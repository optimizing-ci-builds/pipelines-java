# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL"

on: [push]
    # The branches below must be a subset of the branches above

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'java' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
    - uses: actions/setup-python@v2
      with:
        python-version: '3.10'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas
        pip install numpy
    - run: sudo apt update
    - run: sudo apt install inotify-tools
    - run: inotifywait -mr /home/runner/work/pipelines-java/pipelines-java/ --format '%T;%w;%f;%e' --timefmt %T -o /home/runner/inotify-logs.csv & echo 'basak'
    - run: touch starting_analyze_Checkoutrepository_40
    - run: rm starting_analyze_Checkoutrepository_40
    - name: Checkout repository
    - run: touch starting_finished_finished_8979874
    - run: rm starting_finished_finished_8979874
    - uses: jannekem/run-python-script-action@v1
      with:
        script: |
          import pandas as pd
          import numpy as np
          import os
          df = pd.read_csv('/home/runner/inotify-logs.csv', sep = ';', names=['time', 'watched_filename', 'event_filename', 'event_name'])
          df['event_filename'] = df['event_filename'].replace(np.nan, '')
          steps = {}
          starting_indexes = df[(df['event_filename'].str.contains('starting_')) & (df['event_name'] == 'CREATE')].index.to_list() + [df.shape[0]]
          ending_indexes = [0] + df[(df['event_filename'].str.contains('starting_')) & (df['event_name'] == 'DELETE')].index.to_list()
          starting_df = df[df['event_filename'].str.contains('starting_')]
          touch_file_names = ['setup'] + [x.replace('starting_', '') for x in starting_df['event_filename'].value_counts().index.to_list()]
          for starting_index, ending_index, touch_file_name in zip(starting_indexes, ending_indexes, touch_file_names):
              steps[touch_file_name] = (ending_index, starting_index)
          df['watched_filename'] = df['watched_filename'] + df['event_filename']
          df.drop('event_filename', axis=1, inplace=True)
          df.rename(columns={'watched_filename':'file_name'}, inplace=True)
          modify_df = df[df['event_name'] == 'MODIFY']
          file_names = modify_df['file_name'].value_counts().index.to_list()
          info = []
          for file_name in file_names:
              last_access_step = ''
              last_modify_step = ''
              creation_step = ''
              if df[(df['file_name'] == file_name) & (df['event_name'] == 'MODIFY')].shape[0] == 0: last_modify_index = -1
              else: last_modify_index = df[(df['file_name'] == file_name) & (df['event_name'] == 'MODIFY')].index.to_list()[-1]
              last_access_index = 0
              if df[(df['file_name'] == file_name) & (df['event_name'] == 'ACCESS')].shape[0] > 0:
                  last_access_index = df[(df['file_name'] == file_name) & (df['event_name'] == 'ACCESS')].index.to_list()[-1]
              else:
                  last_access_index = -1
                  last_access_step = 'Not provided'
              if last_access_index < last_modify_index:
                  try:
                      creation_index = df[(df['file_name'] == file_name) & (df['event_name'] == 'CREATE')].index.to_list()[0]
                  except:
                      creation_index = -1
                      creation_step = 'Not provided'
                  for touch_file_name, (starting_index, ending_index) in steps.items():
                      if (last_access_index > starting_index) and (last_access_index < ending_index):
                          last_access_step = touch_file_name if touch_file_name == 'setup' else touch_file_name.split('_')[1]
                      if (last_modify_index > starting_index) and (last_modify_index < ending_index):
                          last_modify_step = touch_file_name if touch_file_name == 'setup' else touch_file_name.split('_')[1]
                      if (creation_index > starting_index) and (creation_index < ending_index):
                          creation_step = touch_file_name if touch_file_name == 'setup' else touch_file_name.split('_')[1]
                  if '/home/runner/work/pipelines-java/pipelines-java/.git/' not in file_name:
                      info.append({'file_name': file_name, 'last_access_index': last_access_index, 'last_modify_index': last_modify_index, 'creation_index': creation_index, 'last_access_step':last_access_step , 'last_modify_step':last_modify_step, 'creation_step': creation_step})
          info_df = pd.DataFrame(info)
          step_statistics = []
          for step, (starting_index, ending_index) in steps.items():
              step_name = step if step == 'setup' else step.split('_')[1]
              if step_name == 'finished': continue
              created_file_count = info_df[info_df['creation_step'] == step_name].shape[0]
              created_never_accessed_file_count = info_df[(info_df['creation_step'] == step_name) & (info_df['last_access_index'] == -1)].shape[0]
              modified_files_after_accessed_count = info_df[(info_df['creation_step'] == step_name) & (info_df['last_modify_index'] > info_df['last_access_index'])].shape[0]
              starting_time = list(map(int, df.iloc[starting_index]['time'].split(':')))
              if ending_index == len(df): ending_time = list(map(int, df.iloc[ending_index-1]['time'].split(':')))
              else: ending_time = list(map(int, df.iloc[ending_index]['time'].split(':')))
              hour = ending_time[0] - starting_time[0]
              if starting_time[1] > ending_time[1]:
                  minute = ending_time[1] - starting_time[1] + 60
                  hour -= 1
              else: minute = ending_time[1] - starting_time[1]
              if starting_time[2] > ending_time[2]:
                  second = ending_time[2] - starting_time[2] + 60
                  minute -= 1
              else: second = ending_time[2] - starting_time[2]
              total_seconds = second + (minute * 60) + (hour * 60 * 60)
              if step_name != '':
                  step_statistics.append({'step_name': step_name, 'number_of_created_files': created_file_count,
                   'number_of_files_created_never_accessed': created_never_accessed_file_count, 
                   'number_files_modified_after_accessed': modified_files_after_accessed_count, 'time': total_seconds})
          os.mkdir('optimizing-ci-builds-ci-analysis')
          step_df = pd.DataFrame(step_statistics)
          step_df.to_csv('/home/runner/work/pipelines-java/pipelines-java/optimizing-ci-builds-ci-analysis/steps.csv')
          info_df.to_csv('/home/runner/work/pipelines-java/pipelines-java/optimizing-ci-builds-ci-analysis/files.csv')
    - run: cp /home/runner/inotify-logs.csv /home/runner/work/pipelines-java/pipelines-java/optimizing-ci-builds-ci-analysis/
    - name: Pushes analysis to another repository
      id: push_directory
      uses: cpina/github-action-push-to-another-repository@main
      env:
        API_TOKEN_GITHUB: ${{ secrets.API_TOKEN_GITHUB }}
      with:
        source-directory: 'optimizing-ci-builds-ci-analysis'
        destination-github-username: 'optimizing-ci-builds'
        destination-repository-name: 'ci-analyzes'
        target-directory: 'pipelines-java/1666403741/.github/workflows/codeql-analysis/analyze'
      uses: actions/checkout@v3

    # Initializes the CodeQL tools for scanning.
    - run: touch starting_analyze_InitializeCodeQL_44
    - run: rm starting_analyze_InitializeCodeQL_44
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.
        
        # Details on CodeQL's query packs refer to : https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality

        
    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).
    # If this step fails, then you should remove it and run the build manually (see below)
    - run: touch starting_analyze_Autobuild_58
    - run: rm starting_analyze_Autobuild_58
    - name: Autobuild
      uses: github/codeql-action/autobuild@v2

    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.
    # üìö See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

    #   If the Autobuild fails above, remove it and uncomment the following three lines. 
    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

    # - run: |
    #   echo "Run, Build Application using script"
    #   ./location_of_script_within_repo/buildscript.sh

    - run: touch starting_analyze_PerformCodeQLAnalysis_71
    - run: rm starting_analyze_PerformCodeQLAnalysis_71
    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2

